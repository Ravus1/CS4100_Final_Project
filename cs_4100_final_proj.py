# -*- coding: utf-8 -*-
"""cs_4100_final_proj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TFgs4IzO1D2q0LEzQJcvNHFyQwUVKhIQ
"""

# imports
import pandas as pd
import re
import nltk # splitting data into sentences for NN
from nltk.tokenize import sent_tokenize
import numpy as np
from sklearn.utils.class_weight import compute_class_weight # used to balance classification model
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
import torch
import torchvision
import torchvision.transforms.v2 as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report

# read in data
df_transcript = pd.read_csv('youtube_transcripts.csv')

# remove empty/null transcripts
df_transcript = df_transcript.dropna(subset=['transcript'])
df = df_transcript[df_transcript['transcript'].str.strip() != '']

# remove duplicates
df_transcript = df_transcript.drop_duplicates(subset=['transcript'])

# replace whitespaces
df_transcript['transcript'] = (
    df['transcript']
    .str.replace(r'\s+', ' ', regex=True)
    .str.strip()
)

df_transcript['transcript_clean'] = df_transcript['transcript'].str.lower()

# simple cleaning of transcripts
def clean_transcript_lang(text):
    # remove bracket ([]) tags
    text = re.sub(r'\[[^\]]+\]', ' ', text)
    # remove headers like '>>'
    text = re.sub(r'>+\s*', ' ', text)
    # normalize
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df_transcript['transcript_clean'] = df_transcript['transcript_clean'].apply(clean_transcript_lang)

# tokenize to check transcript lengths
df_transcript['token_count'] = df_transcript['transcript_clean'].str.split().str.len()

df_transcript['token_count'] = df_transcript['transcript_clean'].str.split().str.len()

df_transcript = df_transcript[df_transcript['token_count'] >= 100]

# split lectures into 400 word sections to input for optimized training
def split_lectures(words, max_len=400):
    return ['' ''.join(words[i:i+max_len])
            for i in range(0, len(words), max_len)]

df_transcript['lecture_section'] = df_transcript['transcript_clean'].str.split().apply(split_lectures)

nltk.download('punkt')
nltk.download("punkt_tab")

def smart_sentence_split(transcript):
    # first approach - typical tokenizing
    sentences = nltk.sent_tokenize(transcript)

    # add punctuation if needed
    if len(sentences) <= 1:
        # add breaks at commas, 'so', 'and then', etc.
        # Insert artificial breaks at commas, "so", "and then", etc.
        transcript = re.sub(r'\s*,\s*', '. ', transcript)
        transcript = re.sub(r'\s+so\s+', '. so ', transcript)
        transcript = re.sub(r'\s+and\s+', '. and ', transcript)

        # Re-run tokenizer
        sentences = nltk.sent_tokenize(transcript)

    # Clean extra whitespace
    return [sentence.strip() for sentence in sentences if len(sentence.strip()) > 0]

df_transcript['sentence'] = df_transcript['transcript_clean'].apply(smart_sentence_split)

# combine short sentences
def merge_short_sentences(sentences, min_len=40):
    merged = []
    for s in sentences:
        if not merged:
            merged.append(s)
        elif len(s) < min_len:
            merged[-1] = merged[-1] + " " + s
        else:
            merged.append(s)
    return merged

df_transcript['sentence'] = df_transcript['sentence'].apply(merge_short_sentences)

sentence_entries = []
for idx, row in df_transcript.iterrows():
    for i, sentence in enumerate(row['sentence']):
        sentence_entries.append({
            'transcript_id': idx,
            'sentence_id': i,
            'sentence': sentence,
            'title': row['title'],
            'playlist': row['playlist_name'],
        })

df_sentence = pd.DataFrame(sentence_entries)

df_transcript['playlist_name'].value_counts()

df_transcript["playlist_name"].unique()

# group/cluster/put all topics into buckets
def group_topic(playlist_name: str) -> str:
    name_playlist = playlist_name.lower()

    # NLP/Transformers. Note w is the word(s) that is checked to group the data.
    if any(w in name_playlist for w in [
        'natural language processing', 'nlp', 'transformer', 'bert',
        'spaCy'.lower(), 'hugging face'
    ]):
        return 'NLP'

    # RL
    if 'reinforcement learning' in name_playlist or 'rl ' in name_playlist or 'rl -' in name_playlist:
        return 'RL'

    # ML / Deep Learning
    if any(w in name_playlist for w in [
        'machine learning', 'deep learning', 'neural network',
        'cs229', 'cs230', 'cs221', 'cs234', 'ml tech talks',
        'mit advanced machine learning', 'advanced deep learning',
        'deep learning course', 'deep learning basics',
        'practical deep learning', 'deep learning fundamentals',
        'intro to deep learning', 'deep learning from the foundations'
    ]):
        return 'ML/Deep Learning'

    # AI: General/Papers/Concepts
    if any(w in name_playlist for w in [
        'artificial intelligence', 'ai ', 'ai-', 'ai,', 'ai:',
        'age of a.i.', 'two minute papers', 'papers explained',
        'ai explained', 'cognitive and ai', 'deep learning research papers'
    ]):
        return "AI: General/Papers"

    # DS/Stats
    if any(w in name_playlist for w in [
        'data science', 'pydata', 'scipy', 'tabular data',
        'practical statistics', 'data engineering', 'linear algebra',
        'data analysis', 'pandas tutorial', 'free data science course',
        'python data analysis'
    ]):
        return 'DS/Stats'

    # Programming/Python/Libraries
    if any(w in name_playlist for w in [
        'python', 'django', 'flask', 'pytorch', 'tensorflow',
        'matplotlib', 'modules and libraries', 'projects',
        'programming', 'web scraping', 'beautiful soup'
    ]):
        return 'Programming/Python'

    # Podcasts/Talks
    if any(w in name_playlist for w in [
        'podcast', 'deepmind: the podcast', 'lex fridman',
        'our story'
    ]):
        return 'Podcast/Talk'

    # Everything else
    return 'Other'

df_transcript['topic'] = df_transcript['playlist_name'].apply(group_topic)

df_transcript['topic'].value_counts()

topics = df_transcript["topic"]
classification_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(topics),
    y=topics
)

# just used for data visualization
# df_transcript.head()

# df_sentence.head()

# df_transcript

# df_sentence

df_transcript = df_transcript.reset_index().rename(columns={'index': 'transcript_id'})

# omit outliers
lecture_candidates = df_transcript[
    (df_transcript['token_count'] >= 300) &
    (df_transcript['token_count'] <= 2500)
]

# sampling three lectures per topic
def sample_lectures_per_topic(df):
  lecture_samples = []

  for topic, topic_bucket in df.groupby('topic'):
      lecture_samples_for_topic = topic_bucket.sample(n=3, random_state=42)
      lecture_samples.append(lecture_samples_for_topic)

  return pd.concat(lecture_samples).reset_index(drop=True)

lectures_to_hand_summarize = sample_lectures_per_topic(lecture_candidates)
lectures_to_hand_summarize['paraphrased_summary'] = ''
lectures_to_hand_summarize.to_csv("lectures_to_hand_summarize.csv", index=False)

# read in lectures that were hand summarized
df_summ = pd.read_csv("lectures_hand_summarized.csv")

# create one final dataframe that allows sentences to be invididually evaluated per transcript
df_final = (
    df_sentence
    .merge(df_summ[['transcript_id', 'paraphrased_summary', 'topic']],
           on='transcript_id',
           how='inner')
)
df_final

# max number of sentences for a generated summary - "average" across handwritten summaries
SUMMARY_MAX_LEN = 10

# using tfidf to vectorize sentence importance
tfidf = TfidfVectorizer(max_features=10000,
                        ngram_range=(1, 2),
                        stop_words='english')
tfidf.fit(
    pd.concat([df_final['sentence'],
               df_final['paraphrased_summary']], axis=0)
)

# build setence importance labels
def build_sentence_importance_labels(df):
    labels = []

    for tid, group in df.groupby('transcript_id'):
        sentence_texts = group['sentence'].tolist()
        summary_text = group['paraphrased_summary'].iloc[0]

        sentence_vecs = tfidf.transform(sentence_texts)
        summary_vec  = tfidf.transform([summary_text])

        similarity_scores = cosine_similarity(sentence_vecs, summary_vec).flatten()

        # Top K indices (summary-worthy)
        k = min(SUMMARY_MAX_LEN, len(similarity_scores))
        sentence_idxs = np.argsort(similarity_scores)[-k:]

        y = np.zeros_like(similarity_scores, dtype=int)
        y[sentence_idxs] = 1

        labels.extend(y.tolist())

    df['label'] = labels
    return df

df_final = build_sentence_importance_labels(df_final)
df_final

# checking that there's enough of both. this is neat - this means for about every five sentences, one is actually important.
df_final['label'].value_counts()

y = df_final['label'].to_numpy()
X_tfidf = tfidf.transform(df_final['sentence'])

ids_all = df_final['transcript_id'].unique()
ids_train, ids_rest = train_test_split(ids_all, test_size=0.2, random_state=42)
ids_val, ids_test = train_test_split(ids_rest, test_size=0.50, random_state=42)

mask_train = df_final['transcript_id'].isin(ids_train).to_numpy()
mask_test  = df_final['transcript_id'].isin(ids_test).to_numpy()
mask_val   = df_final['transcript_id'].isin(ids_val).to_numpy()

X_tfidf_train = X_tfidf[mask_train]
X_tfidf_test  = X_tfidf[mask_test]
X_tfidf_val  = X_tfidf[mask_val]
y_train = y[mask_train]
y_test  = y[mask_test]
y_val = y[mask_val]

log_reg = LogisticRegression(
    max_iter=500,
    class_weight='balanced',
    solver='liblinear'
)
log_reg.fit(X_tfidf_train, y_train)

y_pred_val = log_reg.predict(X_tfidf_val)
print(classification_report(y_val, y_pred_val))

y_pred_test = log_reg.predict(X_tfidf_test)
print(classification_report(y_test, y_pred_test))

class FF_Net_Summary(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 1) # single output

    def forward(self, x):
        f1 = F.relu(self.fc1(x))
        f2 = F.relu(self.fc2(f1))
        output = self.fc3(f2)
        return output

device = 'cuda' if torch.cuda.is_available() else 'cpu'

X_tensor_train = torch.tensor(X_tfidf_train.toarray(), dtype=torch.float32).to(device)
X_tensor_test  = torch.tensor(X_tfidf_test.toarray(),  dtype=torch.float32).to(device)
X_tensor_val   = torch.tensor(X_tfidf_val.toarray(),   dtype=torch.float32).to(device)

y_tensor_train = torch.tensor(y_train, dtype=torch.float32).to(device)
y_tensor_test  = torch.tensor(y_test,  dtype=torch.float32).to(device)
y_tensor_val   = torch.tensor(y_val,   dtype=torch.float32).to(device)

input_dim = X_tfidf_train.shape[1]
feedforward_net_summary = FF_Net_Summary(input_dim).to(device)
criterion = nn.CrossEntropyLoss()
optimizer_ffn_summary = optim.SGD(feedforward_net_summary.parameters(), lr=0.01)

batch_size = 64
num_epochs = 25

for epoch in range(num_epochs):
    feedforward_net_summary.train()
    permutation = torch.randperm(X_tensor_train.size(0))

    epoch_loss = 0

    for i in range(0, X_tensor_train.size(0), batch_size):
        idxs = permutation[i:i+batch_size]
        X_selected = X_tensor_train[idxs]
        y_selected = y_tensor_train[idxs]

        optimizer_ffn_summary.zero_grad()

        outputs = feedforward_net_summary(X_selected).squeeze(1)
        loss = criterion(outputs, y_selected)
        loss.backward()
        optimizer_ffn_summary.step()

        epoch_loss += loss.item()

    print(f"Epoch {epoch+1}/{num_epochs} - loss: {epoch_loss:.4f}")

def evaluate(model, X_tensor, y_true):
    model.eval()
    with torch.no_grad():
        outputs = model(X_tensor).squeeze(1)
        probs = torch.sigmoid(outputs).cpu().numpy()
        preds = (probs >= 0.35).astype(int)

    print(classification_report(y_true, preds))
    return preds

print('validation performance')
preds_val = evaluate(feedforward_net_summary, X_tensor_val, y_val)

print('test performance')
preds_test = evaluate(feedforward_net_summary, X_tensor_test, y_test)